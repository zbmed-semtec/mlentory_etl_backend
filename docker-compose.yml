version: '3.8'

services:
  # =============================================================================
  # Core Infrastructure Services
  # =============================================================================
  
  neo4j:
    profiles: ["etl", "api", "complete", "mcp"]
    image: neo4j:${NEO4J_VERSION:-5.14.0}
    container_name: mlentory-neo4j
    ports:
      - "${NEO4J_BROWSER_PORT:-7474}:7474"  # HTTP Browser
      - "${NEO4J_BOLT_PORT:-7687}:7687"      # Bolt protocol
    env_file:
      - .env
    environment:
      # NEO4J_AUTH: neo4j/password
      - NEO4J_PLUGINS=["apoc", "n10s"]
    volumes:
      - ./config/neo4j/data:/data
      - ./config/neo4j/logs:/logs
      - ./config/neo4j/plugins:/plugins
      - ./config/neo4j/import:/var/lib/neo4j/import
      - ./config/neo4j/conf:/conf
    networks:
      - mlentory-network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  elasticsearch:
    profiles: ["etl", "api", "complete", "mcp"]
    image: docker.elastic.co/elasticsearch/elasticsearch:8.18.2
    container_name: mlentory-elasticsearch
    ports:
      - "9201:9200"
      - "9301:9300"
    env_file:
      - .env
    environment:
      - node.name=elastic
      - "transport.host=localhost"
      - "xpack.security.enabled=false"  # Disable security features for development, needs to be enabled for production
      - discovery.type=single-node  #  Use single-node discovery for development
      # - cluster.initial_master_nodes=elastic   # Use this for production
      - cluster.name=es-docker-cluster
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms256m -Xmx256m"
      # - "bootstrap.system_call_filter=false"   # Is not required for version 8.0 and above
      # - node.max_local_storage_nodes=2
      # Disk watermark settings to prevent allocation issues when disk is nearly full
      - "cluster.routing.allocation.disk.watermark.low=95%"  # Start warning at 85% disk usage
      - "cluster.routing.allocation.disk.watermark.high=97%" # High watermark at 97% as requested
      - "cluster.routing.allocation.disk.watermark.flood_stage=98%" # Flood stage at 98%
      - "cluster.info.update.interval=1m" # Update cluster info every minute
    volumes:
      - ./config/elasticsearch/data:/usr/share/elasticsearch/data
    networks:
      - mlentory-network

  # =============================================================================
  # Dagster Orchestration
  # =============================================================================

  dagster-postgres:
    profiles: ["etl", "complete"]
    image: postgres:15-alpine
    container_name: mlentory-dagster-postgres
    environment:
      POSTGRES_USER: ${DAGSTER_POSTGRES_USER:-dagster}
      POSTGRES_PASSWORD: ${DAGSTER_POSTGRES_PASSWORD:-dagster_password_change_me}
      POSTGRES_DB: ${DAGSTER_POSTGRES_DB:-dagster}
    volumes:
      - dagster_postgres_data:/var/lib/postgresql/data
    networks:
      - mlentory-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DAGSTER_POSTGRES_USER:-dagster}"]
      interval: 10s
      timeout: 5s
      retries: 5


  dagster-webserver:
    profiles: ["etl", "complete"]
    build:
      context: .
      dockerfile: deploy/docker/Dockerfile.dagster
    container_name: mlentory-dagster-webserver
    ports:
      - "${DAGSTER_PORT:-3000}:3000"
    env_file:
      - .env
    environment:
      - DAGSTER_HOME=${DAGSTER_HOME:-/opt/dagster/dagster_home}
      - DAGSTER_POSTGRES_USER=${DAGSTER_POSTGRES_USER:-dagster}
      - DAGSTER_POSTGRES_PASSWORD=${DAGSTER_POSTGRES_PASSWORD:-dagster_password_change_me}
      - DAGSTER_POSTGRES_DB=${DAGSTER_POSTGRES_DB:-dagster}
      - DAGSTER_POSTGRES_HOST=dagster-postgres
      - DATA_DIR=${DATA_DIR:-/data}
      - APP_CYPHER_QUERY_MAX_LEN=10000
    volumes:
      - ./etl:/opt/dagster/app/etl
      - ./etl_extractors:/opt/dagster/app/etl_extractors
      - ./etl_transformers:/opt/dagster/app/etl_transformers
      - ./etl_loaders:/opt/dagster/app/etl_loaders
      - ./tests:/opt/dagster/app/tests
      - ./config/etl:/opt/dagster/app/config/etl	
      - ./schemas:/opt/dagster/app/schemas
      - ./data:/data
      - dagster_home:/opt/dagster/dagster_home
      - ./deploy/dagster/dagster.yaml:/opt/dagster/dagster_home/dagster.yaml:ro
      - ./deploy/dagster/workspace.yaml:/opt/dagster/dagster_home/workspace.yaml:ro
    networks:
      - mlentory-network
    depends_on:
      dagster-postgres:
        condition: service_healthy
    command: dagster-webserver -w /opt/dagster/dagster_home/workspace.yaml -h 0.0.0.0 -p 3000

  # =============================================================================
  # MLentory API
  # =============================================================================

  api:
    profiles: ["api", "complete"]
    build:
      context: .
      dockerfile: api/Dockerfile
    container_name: mlentory-api
    ports:
      - "8008:8000"
    env_file:
      - .env
    volumes:
      - ./api:/app/api
      - ./etl_loaders:/app/etl_loaders
      - ./schemas:/app/schemas
    networks:
      - mlentory-network
    depends_on:
      - neo4j
      - elasticsearch
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload

  # =============================================================================
  # MLentory MCP API
  # =============================================================================

  mcp-api:
    profiles: ["mcp"]
    build:
      context: .
      dockerfile: mcp_api/Dockerfile
    container_name: mlentory-mcp-api
    env_file:
      - .env
    volumes:
      - ./mcp_api:/app/mcp_api
      - ./api:/app/api
      - ./etl_loaders:/app/etl_loaders
      - ./etl_extractors:/app/etl_extractors
      - ./schemas:/app/schemas
    networks:
      - mlentory-network
    depends_on:
      - neo4j
      - elasticsearch
    ports:
      - "8009:8009"
    stdin_open: true  # Keep stdin open for MCP stdio communication
    tty: true         # Allocate a pseudo-TTY
    command: python -m mcp_api.server

  ngrok:
    profiles: ["mcp"]
    image: ngrok/ngrok:latest
    container_name: mlentory-ngrok
    # ports:
    #   - "80:80"
    environment:
      - NGROK_AUTHTOKEN=$NGROK_AUTHTOKEN
    command: http --url=believably-graphitic-ann.ngrok-free.dev mlentory-mcp-api:8009
    # network:
    #   - host
    networks:
      - mlentory-network
# =============================================================================
# Networks
# =============================================================================

networks:
  mlentory-network:
    driver: bridge
    name: mlentory-network

# =============================================================================
# Volumes
# =============================================================================

volumes:
  neo4j_data:
    name: mlentory-neo4j-data
  neo4j_logs:
    name: mlentory-neo4j-logs
  neo4j_import:
    name: mlentory-neo4j-import
  neo4j_plugins:
    name: mlentory-neo4j-plugins
  elasticsearch_data:
    name: mlentory-elasticsearch-data
  dagster_postgres_data:
    name: mlentory-dagster-postgres-data
  dagster_home:
    name: mlentory-dagster-home

